{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1wcnjQODYQ4"
   },
   "source": [
    "# ***0. Data Loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n6MJyx0bDlFa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_path = '/Users/akhilmakes/Library/CloudStorage/GoogleDrive-akhil.makes@gmail.com/My Drive/University of Surrey/AI and AI Programming/EEEM005 data for coursework/UNSWNB15_training_coursework.csv'\n",
    "test_1_path = '/Users/akhilmakes/Library/CloudStorage/GoogleDrive-akhil.makes@gmail.com/My Drive/University of Surrey/AI and AI Programming/EEEM005 data for coursework/UNSWNB15_testing1_coursework.csv'\n",
    "test_2_path = '/Users/akhilmakes/Library/CloudStorage/GoogleDrive-akhil.makes@gmail.com/My Drive/University of Surrey/AI and AI Programming/EEEM005 data for coursework/UNSWNB15_testing2_coursework_no_label.csv'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(train_path) # Data used to train the MLP\n",
    "test_1_data = pd.read_csv(test_1_path) # Data used to evaluate the performance of the model\n",
    "test_2_data = pd.read_csv(test_2_path) # Data used to predict the class labels and display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYce1Z49mIsz"
   },
   "source": [
    "# ***1. Data Pre-Processing (Task 1)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1743169338598,
     "user": {
      "displayName": "Akhil Makeswaran",
      "userId": "08831677960342962495"
     },
     "user_tz": 0
    },
    "id": "A0DbTy5EA0f2",
    "outputId": "3fb963d8-e6ef-4723-9da6-70228c948cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Features retain 97.0% of the variance of the data\n",
      "Original Training Data Shape: (20000, 44)\n",
      "Training Data Shape after PCA: (20000, 15)\n"
     ]
    }
   ],
   "source": [
    "# Encode the Categorical Variables - 'proto', 'service', 'state'\n",
    "categorical_columns = ['proto', 'service', 'state']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    unique_values = train_data[col].unique()\n",
    "    mapping = {v: i for i, v in enumerate(unique_values)}\n",
    "    train_data[col] = train_data[col].map(mapping)\n",
    "\n",
    "# Ensure that all the columns in the data are have numbers in them and drop the label column\n",
    "features = train_data.select_dtypes(include=['float64', 'int64']).drop(columns=['label'])\n",
    "\n",
    "# Normalize the values in the data to values between 0 and 1\n",
    "for col in features.columns:\n",
    "    min = features[col].min()\n",
    "    max = features[col].max()\n",
    "    if max - min == 0: ## Checks if the max - min value is 0 to avoid division by 0\n",
    "        features[col] = 0\n",
    "    else:\n",
    "        features[col] = (features[col] - min) / (max - min)\n",
    "\n",
    "# Perform PCA to reduce dimensions of feature space and extract the most useful features\n",
    "# 1. Subtract the data from its mean\n",
    "# 2. Calculate the covariance matrix to measure the relationship between the features\n",
    "# 3. Calculate the Eigenvalues and Eigenvectors from the covariance matrix\n",
    "# 4. Sort by eigenvalue in descending order\n",
    "# (A high eigenvalue indicates that the data is highly spread in that features direction.)\n",
    "# 5. Calculate cumulative variance ratio\n",
    "# (Used to find the number of components that retain a certain variance e.g. 97%)\n",
    "# 6. Reduce the centered data to the k number of useful features\n",
    "\n",
    "mean_sub_features = features - np.mean(features, axis=0)\n",
    "\n",
    "cov_matrix = np.cov(mean_sub_features, rowvar=False)\n",
    "\n",
    "eig_val, eig_vec = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "sort_index = np.argsort(eig_val)[::-1]\n",
    "eig_val = eig_val[sort_index]\n",
    "eig_vec = eig_vec[:, sort_index]\n",
    "\n",
    "cumulative_variance = np.cumsum(eig_val) / np.sum(eig_val)\n",
    "target_variance = 0.97\n",
    "k = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "\n",
    "eigen_vectors_k = eig_vec[:, :k]\n",
    "features_reduced = np.dot(mean_sub_features, eigen_vectors_k)\n",
    "\n",
    "print(f\"{k} Features retain {target_variance * 100}% of the variance of the data\")\n",
    "\n",
    "# Training data is now processed\n",
    "X_train = features_reduced\n",
    "y_train = train_data['label'] # Labels - 0 => Benign, 1 => Malicious\n",
    "\n",
    "# As we reduce the dimensions of the training data, we must also do the same processing to the test data.\n",
    "\n",
    "#testing1\n",
    "for c in categorical_columns:\n",
    "    unique_values = test_1_data[c].unique()\n",
    "    mapping = {v: i for i, v in enumerate(unique_values)}\n",
    "    test_1_data[c] = test_1_data[c].map(mapping)\n",
    "\n",
    "test_1_features = test_1_data.select_dtypes(include=['float64', 'int64']).drop(columns=['label'])\n",
    "\n",
    "for col in test_1_features.columns:\n",
    "    test_min = test_1_features[col].min()\n",
    "    test_max = test_1_features[col].max()\n",
    "    if test_max - test_min == 0:\n",
    "        test_1_features[col] = 0\n",
    "    else:\n",
    "        test_1_features[col] = (test_1_features[col] - test_min) / (test_max - test_min)\n",
    "\n",
    "# Center test data using training mean\n",
    "mean_sub_test = test_1_features - np.mean(features, axis=0)\n",
    "\n",
    "# Project test data using training PCA\n",
    "X_test_1 = np.dot(mean_sub_test, eigen_vectors_k)\n",
    "y_test_1 = test_1_data['label']\n",
    "\n",
    "#testing2\n",
    "for col in categorical_columns:\n",
    "    unique_values = test_2_data[col].unique()\n",
    "    mapping = {v: i for i, v in enumerate(unique_values)}\n",
    "    test_2_data[col] = test_2_data[col].map(mapping)\n",
    "\n",
    "test_2_features = test_2_data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "for c in test_2_features.columns:\n",
    "    test_2_min = test_2_features[c].min()\n",
    "    test_2_max = test_2_features[c].max()\n",
    "\n",
    "    if test_2_max - test_2_min == 0:\n",
    "        test_2_features[c] = 0\n",
    "    else:\n",
    "        test_2_features[c] = (test_2_features[c] - test_2_min) / (test_2_max - test_min)\n",
    "\n",
    "# Center test data using training mean\n",
    "mean_sub_test_2 = test_2_features - np.mean(features, axis=0)\n",
    "\n",
    "# Project test data using training PCA\n",
    "X_test_2 = np.dot(mean_sub_test_2, eigen_vectors_k)\n",
    "\n",
    "print(f\"Original Training Data Shape: {train_data.shape}\")\n",
    "print(f\"Training Data Shape after PCA: {X_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4uQmeM_Albh"
   },
   "source": [
    "# ***2. Model Implementation and Training (Task 2)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "error",
     "timestamp": 1743170133479,
     "user": {
      "displayName": "Akhil Makeswaran",
      "userId": "08831677960342962495"
     },
     "user_tz": 0
    },
    "id": "yzR1gaj0BZlG",
    "outputId": "cec7ab17-4051-4535-aa44-a233e5d3270b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6870\n",
      "Epoch 100: Loss = 0.3375\n",
      "Epoch 200: Loss = 0.3041\n",
      "Epoch 300: Loss = 0.3039\n",
      "Epoch 400: Loss = 0.2907\n",
      "Epoch 500: Loss = 0.2847\n",
      "Epoch 600: Loss = 0.2797\n",
      "Epoch 700: Loss = 0.2759\n",
      "Epoch 800: Loss = 0.2723\n",
      "Epoch 900: Loss = 0.2694\n"
     ]
    }
   ],
   "source": [
    "## MULTI LAYER PERCEPTRON\n",
    "\n",
    "# Define the activation functions and the derivatives which will be used for backpropagation\n",
    "# ReLU is used as it prevents the occurence of vanishing gradients during backpropagation\n",
    "# Sigmoid function is used in the final layer to output a probability\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(x, 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "  return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the input, hidden and output layer sizes\n",
    "input_layer_size = X_train.shape[1]\n",
    "hidden_layer_size = int(input_layer_size * 1.5)\n",
    "output_layer_size = 1\n",
    "\n",
    "# Initialize a random seed for reproducible results\n",
    "np.random.seed(20)\n",
    "\n",
    "# Use He Initialization to assign the weights as we are using the ReLU activation\n",
    "# It prevents the neurons outputting 0\n",
    "\n",
    "## Stage 1 - Input to Hidden Layer\n",
    "w1 = np.random.randn(input_layer_size, hidden_layer_size) * np.sqrt(2 / input_layer_size)\n",
    "b1 = np.zeros((1, hidden_layer_size))\n",
    "## Stage 2 - Hidden Layer 1\n",
    "w2= np.random.randn(hidden_layer_size, hidden_layer_size) * np.sqrt(2 / hidden_layer_size)\n",
    "b2 = np.zeros((1, hidden_layer_size))\n",
    "## Stage 3 - Hidden Layer 2 to Output\n",
    "w3 = np.random.randn(hidden_layer_size, 1) * np.sqrt(2 / hidden_layer_size)\n",
    "b3 = np.zeros((1, output_layer_size))\n",
    "\n",
    "## Forward pass of the network from the input layer to the output layer\n",
    "def forward_pass(x):\n",
    "  z1 = np.dot(x, w1) + b1 ## Multiply the input to the corresponding weights and add the bias term\n",
    "  a1 = relu(z1) ## Apply the ReLU activation function to introduce non-linearity\n",
    "\n",
    "## Hidden Layer 1\n",
    "  z2 = np.dot(a1, w2) + b2 ## Multiply the output of the first hidden layer to the corresponding weights and bias term\n",
    "  a2 = relu(z2) ## Apply the ReLU activation function\n",
    "\n",
    "## Hidden Layer 2\n",
    "  z3 = np.dot(a2, w3) + b3 ## Mulitply the output of the hidden layer with weights and add the bias term\n",
    "  y_prediction = sigmoid(z3) ## Apply the sigmoid activation function to output a probability\n",
    "\n",
    "  return z1, a1, z2, a2, z3, y_prediction\n",
    "\n",
    "## Binary cross entropy loss function as this is a binary classification problem\n",
    "def bce_loss(y_true, y_pred):\n",
    "  N = y_true.shape[0]\n",
    "  loss = -1/N * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1-y_pred))\n",
    "\n",
    "  return np.squeeze(loss) ## Removes singleton dimensions to always produce a scalar value\n",
    "\n",
    "\n",
    "# Backpropagation through the network to calculate gradients w.r.t to the loss.\n",
    "def back_prop(x, y, z1, a1, z2, a2, z3, y_prediction):\n",
    "  m = x.shape[0]\n",
    "\n",
    "# Gradients for the output layer\n",
    "  dz3 = y_prediction - y\n",
    "  dw3 = 1/m * np.dot(a1.T, dz3)\n",
    "  db3 = 1/m * np.sum(dz3, axis=0, keepdims=True)\n",
    "\n",
    "# Gradients for hidden layer 2\n",
    "  da2 = np.dot(dz3, w3.T)\n",
    "  dz2 = da2 * relu_derivative(z2)\n",
    "  dw2 = 1/m * np.dot(a1.T, dz2)\n",
    "  db2 = 1/m * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# Gradients for hidden layer 1\n",
    "  da1 = np.dot(dz2, w2.T)\n",
    "  dz1 = da1 * relu_derivative(z1)\n",
    "  dw1 = 1/m * np.dot(x.T, dz1)\n",
    "  db1 = 1/m * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "  return dw1, db1, dw2, db2, dw3, db3\n",
    "\n",
    "# Function to update the initial weights and biases using a defined learning rate according to the loss\n",
    "def update_gradients(dw1, db1, dw2, db2, dw3, db3, learning_rate):\n",
    "  global w1, b1, w2, b2, w3, b3\n",
    "  w1 -= learning_rate * dw1\n",
    "  b1 -= learning_rate * db1\n",
    "  w2 -= learning_rate * dw2\n",
    "  b2 -= learning_rate * db2\n",
    "  w3 -= learning_rate * dw3\n",
    "  b3 -= learning_rate * db3\n",
    "\n",
    "# The main training loop for the model\n",
    "# default number of epochs is 100 and learning rate is 0.01\n",
    "def train(x, y, epochs=100, learning_rate=0.01):\n",
    "\n",
    "  for i in range(epochs):\n",
    "    z1, a1, z2, a2, z3, y_prediction = forward_pass(x)\n",
    "    loss = bce_loss(y, y_prediction)\n",
    "    dw1, db1, dw2, db2, dw3, db3 = back_prop(x, y, z1, a1, z2, a2, z3, y_prediction)\n",
    "    update_gradients(dw1, db1, dw2, db2, dw3, db3, learning_rate)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "      print(f\"Epoch {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Given a data point X, make a prediction either 0 or 1.\n",
    "def predict(X):\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_pass(X)\n",
    "    return (A3 > 0.5).astype(int)\n",
    "\n",
    "# Train the MLP\n",
    "train(X_train, y_train.values.reshape(-1, 1), epochs=1000, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIM6lyE-BA1B"
   },
   "source": [
    "# ***3. Model Performance Evaluation (Task 3)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "filj8Tb2Bav9",
    "outputId": "c98781f7-1f4d-44ed-9c34-e20d7d3b679d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Performance Evaluation-------\n",
      "\n",
      "Accuracy: 93.22 %\n",
      "Precision: 0.94\n",
      "Recall: 0.97\n",
      "F1 Score: 0.95\n",
      "\n",
      "-------Class Predictions-------\n",
      "\n",
      "UNSWNB15_testing1:\n",
      "Prediction: [1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1]\n",
      "Ground Truth: [1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1]\n",
      "\n",
      "UNSWNB15_testing2:\n",
      "Prediction: [1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_1 = predict(X_test_1) # Model Prediction on testing1\n",
    "y_true_1 = y_test_1.values.reshape(-1, 1) # Ground truth values for testing1\n",
    "\n",
    "y_pred_2 = predict(X_test_2) # Model prediction on testing2\n",
    "\n",
    "def evaluate(predictions, true_values):\n",
    "\n",
    "    accuracy = np.mean(predictions == true_values) * 100 # Accuracy of the model as a percentage\n",
    "\n",
    "    predictions = predictions.flatten()\n",
    "    true_values = true_values.flatten()\n",
    "\n",
    "    true_pos = np.sum((predictions == 1) & (true_values == 1)) # Number of malicious packet that were predicted as malicious\n",
    "    false_pos = np.sum((predictions == 1) & (true_values == 0)) # Number of malicious packets that were predicted as benign\n",
    "    false_neg = np.sum((predictions == 0) & (true_values == 1)) # Number of benign packets that were predicted as malicious\n",
    "\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "accuracy, precision, recall, f1_score = evaluate(y_pred_1, y_true_1)\n",
    "\n",
    "print(\"-------Performance Evaluation-------\\n\")\n",
    "print(f\"Accuracy: {round(accuracy,2)} %\")\n",
    "print(f\"Precision: {round(precision,2)}\")\n",
    "print(f\"Recall: {round(recall,2)}\")\n",
    "print(f\"F1 Score: {round(f1_score,2)}\\n\")\n",
    "\n",
    "print(\"-------Class Predictions-------\\n\")\n",
    "print(\"UNSWNB15_testing1:\")\n",
    "print(f\"Prediction: {y_pred_1[:25].flatten()}\") # Display the first 25 predictions for testing1\n",
    "print(f\"Ground Truth: {y_true_1[:25].flatten()}\\n\")\n",
    "print(\"UNSWNB15_testing2:\")\n",
    "print(f\"Prediction: {y_pred_2.flatten()}\")# Display the predictions for testing2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iCUL48FBwlQ"
   },
   "source": [
    "# ***4. Performance Evaluation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hda1q3KOBM_f"
   },
   "source": [
    "**Please report the classification accuracy your model achieved on data samples in the testing set 1 below (in percentage)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePqjGPnHmYWv"
   },
   "source": [
    "93.22%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpMWHu9bCFSo"
   },
   "source": [
    "**Please provide the predicted class labels of the data samples in the testing set 2 below (0 or 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_8B_MxvCnf3"
   },
   "source": [
    "| **Sample ID** |**Predicted Label** |\n",
    "| --- | --- |\n",
    "| 1 |  1 |\n",
    "| 2 | 0  |\n",
    "| 3 |  1 |\n",
    "| 4 | 1  |\n",
    "| 5 |  0 |\n",
    "| 6 |  1 |\n",
    "| 7 |  0 |\n",
    "| 8 |  0 |\n",
    "| 9 |  1 |\n",
    "| 10 | 0  |\n",
    "| 11 | 0  |\n",
    "| 12 | 1 |\n",
    "| 13 |  1 |\n",
    "| 14 |  1 |\n",
    "| 15 |  1 |\n",
    "| 16 | 1  |\n",
    "| 17 | 1  |\n",
    "| 18 | 1  |\n",
    "| 19 | 0  |\n",
    "| 20 | 1  |\n",
    "| 21 | 1  |\n",
    "| 22 | 1  |\n",
    "| 23 |  1 |\n",
    "| 24 | 1  |\n",
    "| 25 | 1  |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
